{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ac543d-6ed4-4850-9c73-aa81d5ad2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f220037f-8a76-4c1e-abcd-cdd3f2571d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.11\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/taiwosokunbi/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e28884-0a08-44ab-9c06-c5c73d277088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7602f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of matter and energy on the smallest scales of atoms and subatomic particles, where particles can exist in multiple states simultaneously and are subject to probabilistic outcomes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e58a711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
      "\n",
      "class ChatOpenAI(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatOpenAI(*, name: Optional[str] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[pydantic.v1.types.SecretStr] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None) -> None\n",
      " |  \n",
      " |  `OpenAI` Chat large language models API.\n",
      " |  \n",
      " |  To use, you should have the\n",
      " |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      " |  \n",
      " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      " |  in, even if not explicitly saved on this class.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.chat_models import ChatOpenAI\n",
      " |          openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind functions (and other objects) to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI function-calling API.\n",
      " |      \n",
      " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
      " |          `function_call` request parameters are officially marked as deprecated by\n",
      " |          OpenAI.\n",
      " |      \n",
      " |      Args:\n",
      " |          functions: A list of function definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      " |              models and callables will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          function_call: Which function to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any).\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none'], bool]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |      \n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
      " |              models, callables, and BaseTools will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          tool_choice: Which tool to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any), or a dict of the form:\n",
      " |              {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      " |      \n",
      " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
      " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |  \n",
      " |  with_structured_output(self, schema: 'Optional[_DictOrPydanticClass]' = None, *, method: \"Literal['function_calling', 'json_mode']\" = 'function_calling', include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, _DictOrPydantic]'\n",
      " |      [*Beta*] Model wrapper that returns outputs formatted to match the given schema.\n",
      " |      \n",
      " |              Args:\n",
      " |                  schema: The output schema as a dict or a Pydantic class. If a Pydantic class\n",
      " |                      then the model output will be an object of that class. If a dict then\n",
      " |                      the model output will be a dict. With a Pydantic class the returned\n",
      " |                      attributes will be validated, whereas with a dict they will not be. If\n",
      " |                      `method` is \"function_calling\" and `schema` is a dict, then the dict\n",
      " |                      must match the OpenAI function-calling spec.\n",
      " |                  method: The method for steering model generation, either \"function_calling\"\n",
      " |                      or \"json_mode\". If \"function_calling\" then the schema will be converted\n",
      " |                      to an OpenAI function and the returned model will make use of the\n",
      " |                      function-calling API. If \"json_mode\" then OpenAI's JSON mode will be\n",
      " |                      used. Note that if using \"json_mode\" then you must include instructions\n",
      " |                      for formatting the output into the desired schema into the model call.\n",
      " |                  include_raw: If False then only the parsed structured output is returned. If\n",
      " |                      an error occurs during model output parsing it will be raised. If True\n",
      " |                      then both the raw model response (a BaseMessage) and the parsed model\n",
      " |                      response will be returned. If an error occurs during output parsing it\n",
      " |                      will be caught and returned as well. The final output is always a dict\n",
      " |                      with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
      " |      \n",
      " |              Returns:\n",
      " |                  A Runnable that takes any ChatModel input and returns as output:\n",
      " |      \n",
      " |                      If include_raw is True then a dict with keys:\n",
      " |                          raw: BaseMessage\n",
      " |                          parsed: Optional[_DictOrPydantic]\n",
      " |                          parsing_error: Optional[BaseException]\n",
      " |      \n",
      " |                      If include_raw is False then just _DictOrPydantic is returned,\n",
      " |                      where _DictOrPydantic depends on the schema:\n",
      " |      \n",
      " |                      If schema is a Pydantic class then _DictOrPydantic is the Pydantic\n",
      " |                          class.\n",
      " |      \n",
      " |                      If schema is a dict then _DictOrPydantic is a dict.\n",
      " |      \n",
      " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=False):\n",
      " |                  .. code-block:: python\n",
      " |      \n",
      " |                      from langchain_openai import ChatOpenAI\n",
      " |                      from langchain_core.pydantic_v1 import BaseModel\n",
      " |      \n",
      " |                      class AnswerWithJustification(BaseModel):\n",
      " |                          '''An answer to the user question along with justification for the answer.'''\n",
      " |                          answer: str\n",
      " |                          justification: str\n",
      " |      \n",
      " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
      " |                      structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
      " |      \n",
      " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |      \n",
      " |                      # -> AnswerWithJustification(\n",
      " |                      #     answer='They weigh the same',\n",
      " |                      #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |                      # )\n",
      " |      \n",
      " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=True):\n",
      " |                  .. code-block:: python\n",
      " |      \n",
      " |                      from langchain_openai import ChatOpenAI\n",
      " |                      from langchain_core.pydantic_v1 import BaseModel\n",
      " |      \n",
      " |                      class AnswerWithJustification(BaseModel):\n",
      " |                          '''An answer to the user question along with justification for the answer.'''\n",
      " |                          answer: str\n",
      " |                          justification: str\n",
      " |      \n",
      " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
      " |                      structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n",
      " |      \n",
      " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |                      # -> {\n",
      " |                      #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
      " |                      #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
      " |                      #     'parsing_error': None\n",
      " |                      # }\n",
      " |      \n",
      " |              Example: Function-calling, dict schema (method=\"function_calling\", include_raw=False):\n",
      " |                  .. code-block:: python\n",
      " |      \n",
      " |                      from langchain_openai import ChatOpenAI\n",
      " |                      from langchain_core.pydantic_v1 import BaseModel\n",
      " |                      from langchain_core.utils.function_calling import convert_to_openai_tool\n",
      " |      \n",
      " |                      class AnswerWithJustification(BaseModel):\n",
      " |                          '''An answer to the user question along with justification for the answer.'''\n",
      " |                          answer: str\n",
      " |                          justification: str\n",
      " |      \n",
      " |                      dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
      " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
      " |                      structured_llm = llm.with_structured_output(dict_schema)\n",
      " |      \n",
      " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
      " |                      # -> {\n",
      " |                      #     'answer': 'They weigh the same',\n",
      " |                      #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |                      # }\n",
      " |      \n",
      " |              Example: JSON mode, Pydantic schema (method=\"json_mode\", include_raw=True):\n",
      " |                  .. code-block::\n",
      " |      \n",
      " |                      from langchain_openai import ChatOpenAI\n",
      " |                      from langchain_core.pydantic_v1 import BaseModel\n",
      " |      \n",
      " |                      class AnswerWithJustification(BaseModel):\n",
      " |                          answer: str\n",
      " |                          justification: str\n",
      " |      \n",
      " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
      " |                      structured_llm = llm.with_structured_output(\n",
      " |                          AnswerWithJustification,\n",
      " |                          method=\"json_mode\",\n",
      " |                          include_raw=True\n",
      " |                      )\n",
      " |      \n",
      " |                      structured_llm.invoke(\n",
      " |                          \"Answer the following question. \"\n",
      " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
      " |      \n",
      " |      \"\n",
      " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |                      )\n",
      " |                      # -> {\n",
      " |                      #     'raw': AIMessage(content='{\n",
      " |          \"answer\": \"They are both the same weight.\",\n",
      " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
      " |      }'),\n",
      " |                      #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
      " |                      #     'parsing_error': None\n",
      " |                      # }\n",
      " |      \n",
      " |              Example: JSON mode, no schema (schema=None, method=\"json_mode\", include_raw=True):\n",
      " |                  .. code-block::\n",
      " |      \n",
      " |                      from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |                      structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
      " |      \n",
      " |                      structured_llm.invoke(\n",
      " |                          \"Answer the following question. \"\n",
      " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
      " |      \n",
      " |      \"\n",
      " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |                      )\n",
      " |                      # -> {\n",
      " |                      #     'raw': AIMessage(content='{\n",
      " |          \"answer\": \"They are both the same weight.\",\n",
      " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
      " |      }'),\n",
      " |                      #     'parsed': {\n",
      " |                      #         'answer': 'They are both the same weight.',\n",
      " |                      #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n",
      " |                      #     },\n",
      " |                      #     'parsing_error': None\n",
      " |                      # }\n",
      " |      \n",
      " |      \n",
      " |              \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.v1.main.ModelMetaclass\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |  \n",
      " |  get_lc_namespace() -> 'List[str]' from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |  \n",
      " |  is_lc_serializable() -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Return whether this model can be serialized by Langchain.\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain_openai.chat_models.base.ChatOpenAI.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'async_client': 'Any', 'client': 'Any', 'default_he...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'async_client': True, 'callback_manager': True, ...\n",
      " |  \n",
      " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function ChatOpenAI.build_extra>]\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...Non...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a7962e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics be di branch of physics wey dey study small small particles like atoms and molecules and how dem behave for inside di universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a teacher  and respond in Pidgin'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadaeb8",
   "metadata": {},
   "source": [
    "#### Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95473b",
   "metadata": {},
   "source": [
    "In-memory Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70c8ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e8f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0652bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 3.25 ms, total: 24.2 ms\n",
      "Wall time: 21.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the Muslim chicken cross the road?\\n\\nTo get to the other mosque!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that a muslim would understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e70fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 362 s, sys: 35 s, total: 397 s\n",
      "Wall time: 401 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the Muslim chicken cross the road?\\n\\nTo get to the other mosque!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46814828",
   "metadata": {},
   "source": [
    "#### SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63dd2367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='.langchain.db'))\n",
    "llm.invoke('Tell me a joke')\n",
    "llm.invoke('Tell me joke')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edde04",
   "metadata": {},
   "source": [
    "#### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "252dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Make Doughnuts:\n",
      "\n",
      "Ingredients:\n",
      "- 2 1/4 tsp active dry yeast\n",
      "- 1/4 cup warm water\n",
      "- 3/4 cup warm milk\n",
      "- 1/4 cup granulated sugar\n",
      "- 1/2 tsp salt\n",
      "- 1/4 cup unsalted butter, melted\n",
      "- 2 large eggs\n",
      "- 4 cups all-purpose flour\n",
      "- Vegetable oil for frying\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, dissolve the yeast in warm water and let it sit for 5 minutes until it becomes frothy.\n",
      "\n",
      "2. In a large mixing bowl, combine the warm milk, sugar, salt, melted butter, eggs, and yeast mixture.\n",
      "\n",
      "3. Gradually add the flour to the wet ingredients, stirring until a sticky dough forms.\n",
      "\n",
      "4. Turn the dough out onto a floured surface and knead for 5-7 minutes, adding more flour as needed until the dough is smooth and elastic.\n",
      "\n",
      "5. Place the dough in a greased bowl, cover with a clean kitchen towel, and let it rise in a warm place for 1-2 hours, or until doubled in size.\n",
      "\n",
      "6. Once the dough has risen, punch it down and roll it out on a floured surface to 1/2 inch thickness.\n",
      "\n",
      "7. Use a doughnut cutter or a round cookie cutter to cut out doughnut shapes. Use a smaller cutter to make the center holes.\n",
      "\n",
      "8. Place the doughnuts on a floured baking sheet, cover with a towel, and let them rise for another 30-45 minutes.\n",
      "\n",
      "9. Heat vegetable oil in a deep fryer or large pot to 375F (190C).\n",
      "\n",
      "10. Carefully place the doughnuts in the hot oil, frying for 1-2 minutes on each side until golden brown.\n",
      "\n",
      "11. Remove the doughnuts from the oil and place them on a paper towel-lined plate to drain.\n",
      "\n",
      "12. While the doughnuts are still warm, you can coat them in sugar, glaze, or icing of your choice.\n",
      "\n",
      "13. Enjoy your freshly made doughnuts!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a how to make doughnut.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10720a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make doughnuts at home, you will need the following ingredients:\n",
      "\n",
      "- 2 1/4 teaspoons active dry yeast\n",
      "- 1/4 cup warm water (110F)\n",
      "- 3/4 cup warm milk (110F)\n",
      "- 1/4 cup granulated sugar\n",
      "- 1/2 teaspoon salt\n",
      "- 1/4 cup unsalted butter, melted\n",
      "- 1 egg\n",
      "- 3 cups all-purpose flour\n",
      "- Oil for frying\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, dissolve the yeast in the warm water. Let it sit for about 5 minutes until it becomes frothy.\n",
      "\n",
      "2. In a large mixing bowl, combine the warm milk, sugar, salt, melted butter, and egg. Add the yeast mixture and stir to combine.\n",
      "\n",
      "3. Gradually add the flour, stirring until a soft dough forms. Knead the dough on a floured surface for about 5 minutes, until it becomes smooth and elastic.\n",
      "\n",
      "4. Place the dough in a greased bowl, cover with a clean kitchen towel, and let it rise in a warm place for about 1 hour, or until it has doubled in size.\n",
      "\n",
      "5. Punch down the dough and roll it out on a floured surface to about 1/2 inch thickness. Use a doughnut cutter or a round cookie cutter to cut out doughnut shapes.\n",
      "\n",
      "6. Place the doughnuts on a baking sheet lined with parchment paper and cover with a kitchen towel. Let them rise for about 30 minutes.\n",
      "\n",
      "7. Heat oil in a deep fryer or a heavy-bottomed pot to 375F. Carefully add the doughnuts, a few at a time, and fry for about 1-2 minutes on each side, until they are golden brown.\n",
      "\n",
      "8. Remove the doughnuts with a slotted spoon and place them on a paper towel-lined plate to drain.\n",
      "\n",
      "9. Allow the doughnuts to cool slightly before glazing or topping with your favorite toppings, such as powdered sugar, cinnamon sugar, or chocolate glaze.\n",
      "\n",
      "Enjoy your homemade doughnuts!"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623bcae",
   "metadata": {},
   "source": [
    "###Prompt Templates\n",
    "\n",
    "- A prompt refers to the input to model\n",
    "- Prompt templates are a way to create dynamic prompts for LLMs\n",
    "- A prompt template takes a piece of text and injects a user's inut into that piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eec750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You are an experience virologist.\\nWrite a few sentences about the following \"Hepatitis\" in pidgin\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ''' You are an experience virologist.\n",
    "Write a few sentences about the following \"{virus}\" in {language}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(virus='Hepatitis', language='pidgin')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82c86aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatitis na sickness wey dey affect liver. E fit dey caused by virus, alcohol, or drugs. E dey important make person dey avoid risky behaviors wey fit lead to hepatitis."
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "# output = llm.invoke(prompt)\n",
    "# print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a87ab7",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates\n",
    "- ChatPromptTemplates are specifically designed for tasks that involve in engaging conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ebc1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You respond in the JSON format\n",
      "Human: Top 5 countries in Africa by population.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond in the JSON format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format(n='5', area='Africa')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d059f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"name\": \"Nigeria\",\n",
      "            \"population\": \"206,139,589\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Ethiopia\",\n",
      "            \"population\": \"114,963,588\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Egypt\",\n",
      "            \"population\": \"102,334,404\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"DR Congo\",\n",
      "            \"population\": \"89,561,403\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Tanzania\",\n",
      "            \"population\": \"66,691,832\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3aa1a4",
   "metadata": {},
   "source": [
    "#### Chains\n",
    "Chains are like series of steps and actions. And each step can involve talking language models using a tool.\n",
    "Chains allow us to combine multiple components together to solve a specific task and build an entire LLM application.\n",
    "Langchain provides a standard interface for chains and some implementation to make it easy to use chains.\n",
    "Example of chains are:\n",
    "- Simple chain\n",
    "- Sequential chain\n",
    "- Custom chain\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77717c",
   "metadata": {},
   "source": [
    "#### Simple chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f362fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an experience virologist.\n",
      "Write a few sentences about the following \"Ebola\" in english\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'language': 'english', 'virus': 'Ebola', 'text': 'Ebola virus disease (EVD) is a severe and often deadly illness caused by the Ebola virus. The virus is transmitted to people from wild animals and spreads in the human population through human-to-human transmission. Symptoms of Ebola can include fever, severe headache, muscle pain, fatigue, diarrhea, vomiting, and in some cases, internal and external bleeding. Outbreaks of Ebola have occurred in Africa, with the most recent outbreak in the Democratic Republic of the Congo being declared over in June 2020. Preventing the spread of Ebola involves early detection, isolation of cases, contact tracing, and safe burial practices.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = ''' You are an experience virologist.\n",
    "Write a few sentences about the following \"{virus}\" in {language}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'language': 'english', 'virus': 'Ebola'})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9743e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter country name:  Ethiopia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of a Ethiopia? List top 5 places to visit in the city. Use bullet points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Ethiopia is Addis Ababa.\n",
      "\n",
      "Top 5 places to visit in Addis Ababa:\n",
      "- National Museum of Ethiopia\n",
      "- Holy Trinity Cathedral\n",
      "- Ethnological Museum\n",
      "- Mount Entoto\n",
      "- Merkato Market\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "template = 'What is the capital of a {country}? List top 5 places to visit in the city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter country name: ')\n",
    "output = chain.invoke(country)\n",
    "\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07321631",
   "metadata": {},
   "source": [
    "#### Sequential Chain\n",
    "With sequential chains, you can make a series of calls to one more LLMs. You can take the output from one chain and \n",
    "use it as input to another chain.\n",
    "- Simple sequential chain - represent a series of chains, where each individual chain has a single input and a single output,\n",
    "  and output of one is used as input for the next step.\n",
    "- General form of sequential chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aaded04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mA thread in Python is a separate flow of execution within a program. Threads allow a program to perform multiple tasks concurrently, which can improve performance and responsiveness. \n",
      "\n",
      "In Python, threads are created using the `threading` module. To create a thread, you need to define a function that represents the task you want the thread to perform, and then create a `Thread` object, passing the function as an argument. \n",
      "\n",
      "Here is an example of a simple function that creates and starts a thread:\n",
      "\n",
      "```python\n",
      "import threading\n",
      "\n",
      "def my_function():\n",
      "    print(\"This is running in a separate thread\")\n",
      "\n",
      "# Create a Thread object\n",
      "my_thread = threading.Thread(target=my_function)\n",
      "\n",
      "# Start the thread\n",
      "my_thread.start()\n",
      "\n",
      "# Wait for the thread to finish\n",
      "my_thread.join()\n",
      "```\n",
      "\n",
      "In this example, `my_function` is the function that represents the task we want the thread to perform. We create a `Thread` object called `my_thread` and pass `my_function` as the `target` argument. We then start the thread using the `start` method and wait for it to finish using the `join` method.\n",
      "\n",
      "It's important to note that threads share the same memory space, so you need to be careful when accessing shared resources to avoid race conditions. Python also has a Global Interpreter Lock (GIL) that prevents multiple threads from executing Python bytecode at the same time, which can limit the effectiveness of using threads for parallel processing.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThreads in Python offer a way to achieve concurrency in a program by allowing multiple tasks to run concurrently. By creating threads, different parts of the program can be executed simultaneously, potentially improving performance and responsiveness. \n",
      "\n",
      "The `threading` module in Python provides the necessary functionality to work with threads. Threads are created by defining a function that represents the task to be performed by the thread and then encapsulating that function within a `Thread` object. Some important concepts related to working with threads in Python include:\n",
      "\n",
      "1. **Creating a Thread:** A thread is created by instantiating a `Thread` object. This object takes a `target` argument, which specifies the function that the thread should execute.\n",
      "\n",
      "2. **Starting a Thread:** Once a thread object is created, the `start` method is called on the object to initiate the execution of the associated function in a separate thread.\n",
      "\n",
      "3. **Joining a Thread:** The `join` method is used to wait for the completion of a thread. This is important for ensuring that the main program does not proceed until the thread completes its task.\n",
      "\n",
      "4. **Thread Safety:** Since threads share the same memory space, caution must be exercised when accessing shared data to prevent race conditions. Race conditions occur when multiple threads attempt to access and modify shared resources simultaneously, potentially leading to unpredictable behavior.\n",
      "\n",
      "5. **Global Interpreter Lock (GIL):** The GIL in Python is a mutex that allows only one thread to execute Python bytecode at a time. This can limit the effectiveness of using threads for parallel processing in Python. As a result, Python threads may not be well-suited for CPU-bound tasks that require high levels of parallelism.\n",
      "\n",
      "While threads provide a means to introduce concurrency in a Python program, it is important to consider the limitations imposed by the GIL and design threads carefully to handle shared resources safely. Alternatively, Python offers other concurrency paradigms like multiprocessing and asynchronous programming using coroutines to work around the limitations of threads in Python.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced python programmer. Write a function that explains the concept of {concept}'\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(llm=llm1, prompt=prompt_template_1)\n",
    "\n",
    "\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    template='Given the function{function}. explain it as deep as possible'\n",
    ")\n",
    "\n",
    "llm_2 =ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "chain_2 = LLMChain(llm=llm_2, prompt=prompt_template_2)\n",
    "\n",
    "overall_chain  = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('thread')\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ff6c4",
   "metadata": {},
   "source": [
    "#### Langchain Agents\n",
    "Large Language Models are powerful but lack the ability to perform certain tasks that even the simplest of apllication can do easiy.\n",
    "For example, LLMS struggle with calculation, logic or communicating other external components. the solution to this problem is to use agents.\n",
    "Agents are enabling tool for LLMs.\n",
    "Agents combine langchain tools and chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb3ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 156, 162, 168, 174, 180, 186, 192, 198]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n*2 for n in range(100) if n%3 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc804c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe need to calculate the factorial of 12 first, then find the square root of that result. We can use the math module in Python to help us with this calculation.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe imported the math module successfully, now let's calculate the factorial of 12.\n",
      "Action: Python_REPL\n",
      "Action Input: math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe factorial of 12 is 479001600. Now, let's find the square root of this number and display it in 4 decimal places.\n",
      "Action: Python_REPL\n",
      "Action Input: round(math.sqrt(math.factorial(12)), 4)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe square root of the factorial of 12, rounded to 4 decimal places, is approximately 1947.7346.\n",
      "Final Answer: 1947.7346\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of factorial of 12 and display it in a 4 decimal places',\n",
       " 'output': '1947.7346'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "agent_excutor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "agent_excutor.invoke('Calculate the square root of factorial of 12 and display it in a 4 decimal places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af533bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mPermutation of 5 means arranging 5 elements in a specific order. We can calculate this using the math module in Python.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe math module is now imported, we can use it to calculate the permutation.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems like there was a typo in the action input. The correct function to use for permutation in the math module is `math.perm()`.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe correct function is `math.perm(n, k)` where n is the total number of elements and k is the number of elements to choose. In this case, n = 5 and k = 5.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe permutation of 5 elements is calculated to be 120.\n",
      "Final Answer: 120\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Calculate the permutation 0f 5', 'output': '120'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_excutor.invoke('Calculate the permutation 0f 5')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76734a81",
   "metadata": {},
   "source": [
    "#### langchain tools: DuckDuckgo and Wikipedia\n",
    "Langchain tools are like spcialized apps for your LLM. They are tiny code modules that allow it to access information.\n",
    "These tools are used to connect LLMs to search engines, databases, APIs, etc thereby expanding its knowledge and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b583b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212c438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn how to make whipped cream from scratch with different methods, tips, and tricks. Whether you use a whisk, a mixer, a blender, or a shaker, you can achieve whipped cream perfection in minutes. Learn how to make homemade whipped cream with heavy cream, sugar, and a few tips and tricks. Find out how to flavor, store, and stabilize whipped cream for different desserts. When ready, measure it into the bowl of a stand mixer with a whisk attachment or a metal bowl. Add the vanilla and other flavorings. Begin whipping. Start beating at medium speed. For the first several minutes, the cream will be very frothy and bubbly. Watch for trails in the cream (4 to 5 minutes). Instructions. Place the mixing bowl and whisk attachment in the freezer for 5 to 10 minutes to chill. Pour the heavy cream into the chilled bowl and use an electric or stand mixer to beat the heavy cream on medium-high speed until soft peaks form. Slowly add the powdered sugar and vanilla extract. Directions. In a medium bowl, combine cream and confectioners' sugar. Place over a large bowl filled with ice. Using a whisk, whip until stiff peaks form. With our best tips, 2 ingredients ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search  = DuckDuckGoSearchRun()\n",
    "output = search.invoke('how to make whip cream')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86dd03ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb11a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f7bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Freddie Mercury (born September 5, 1946, Stone Town, Zanzibar [now in Tanzania]died November 24, 1991, Kensington, London, England) British rock singer and songwriter whose flamboyant showmanship and powerfully agile vocals, most famously for the band Queen, made him one of rock's most dynamic front men.. Bulsara was born to Parsi parents who had emigrated from India to Zanzibar, where ..., title: Freddie Mercury | Biography, Parents, Songs, & Facts, link: https://www.britannica.com/biography/Freddie-Mercury], [snippet: Freddie Mercury always talked about his pride at writing \"Killer Queen,\" which appeared on Queen's 1974 album Sheer Heart Attack. He said it was written in one night and was a song that he ..., title: Best Freddie Mercury Songs: 20 Essential Solo And Queen Tracks, link: https://www.udiscovermusic.com/stories/best-freddie-mercury-songs/], [snippet: Freddie Mercury Facts: 15 Things You Never Knew About The Queen Frontman 1: Freddie Mercury was a brilliant boxer. At school in India, the young Freddie Mercury was a good table tennis player. He ..., title: Freddie Mercury Facts: Things You Never Knew About Queen's Frontman, link: https://www.udiscovermusic.com/stories/freddie-mercury-facts-you-need-to-know/], [snippet: Queen frontman Freddie Mercury and the enduring love he had for his best friend and 'soulmate', Mary Austin, has become one of the most famous - and unusual - love stories in British music history. The singer's ex-fiance became the Queen star's confidante and life partner, and was the person to whom Freddie Mercury dedicated the song ..., title: Freddie Mercury: Mary Austin breaks silence on relationship with Queen ..., link: https://www.smoothradio.com/artists/freddie-mercury/mary-austin-interview-relationship/]\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search  = DuckDuckGoSearchResults()\n",
    "# output = search.invoke('how to make whip cream step by step')\n",
    "output = search.run('Fred Mercury and QUeen')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d93b8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: The White Tower is one of the world's most famous castles and a recognizable London landmark. Inside, you'll find the 350-year-old exhibition, \"Line of Kings,\" which includes suits of armor worn ..., title: 34 Best Things to Do in London, England | U.S. News Travel, link: https://travel.usnews.com/London_England/Things_To_Do/], [snippet: Discover the best attractions, events and activities in London with this ultimate guide. Whether you want to see iconic landmarks, explore museums, enjoy fun days out or find free things to do, London has it all., title: The 101 best things to do in London - updated for 2024 - visitlondon.com, link: https://www.visitlondon.com/things-to-do/101-things-to-do-in-london], [snippet: Plan your trip to London with this guide to the best tourist attractions, from the iconic London Eye to the historic Tower of London. Book tickets online for the top 10 bookable attractions, or enjoy free museums and galleries., title: 10 unmissable London attractions to visit in 2024 and 2025, link: https://www.visitlondon.com/things-to-do/sightseeing/london-attraction/top-ten-attractions], [snippet: Discover the best experiences in London, from historic landmarks and art galleries to vibrant neighborhoods and cultural events. Learn about the city's rich and diverse history, culture and cuisine with tips and recommendations from Lonely Planet experts., title: 13 of the best things to do in London - Lonely Planet, link: https://www.lonelyplanet.com/articles/top-things-to-do-in-london]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='en-en', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('London')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'snippet'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
