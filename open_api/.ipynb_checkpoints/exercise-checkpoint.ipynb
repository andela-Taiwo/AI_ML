{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4ac543d-6ed4-4850-9c73-aa81d5ad2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c062c-b6a5-4204-9e7a-a554f883c732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f220037f-8a76-4c1e-abcd-cdd3f2571d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.11\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/taiwosokunbi/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: jupyter_ai_magics, langchain-experimental\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40e28884-0a08-44ab-9c06-c5c73d277088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7602f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of subatomic particles using principles of wave-particle duality and probabilistic outcomes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e58a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a7962e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics na di study of di behavior of small particles like atoms and subatomic particles wey dey follow different rules from di classical physics wey we sabi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a teacher  and respond in Pidgin'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadaeb8",
   "metadata": {},
   "source": [
    "#### Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95473b",
   "metadata": {},
   "source": [
    "In-memory Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70c8ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61e8f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0652bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 6.42 ms, total: 21.2 ms\n",
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the Muslim chicken cross the road?\\n\\nTo get to the other mosque!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that a muslim would understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e70fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 474 µs, sys: 60 µs, total: 534 µs\n",
      "Wall time: 610 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the Muslim chicken cross the road?\\n\\nTo get to the other mosque!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46814828",
   "metadata": {},
   "source": [
    "#### SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63dd2367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='.langchain.db'))\n",
    "llm.invoke('Tell me a joke')\n",
    "llm.invoke('Tell me joke')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edde04",
   "metadata": {},
   "source": [
    "#### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "252dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Make Doughnuts:\n",
      "\n",
      "Ingredients:\n",
      "- 2 1/4 tsp active dry yeast\n",
      "- 1/4 cup warm water\n",
      "- 3/4 cup warm milk\n",
      "- 1/4 cup granulated sugar\n",
      "- 1/2 tsp salt\n",
      "- 1/4 cup unsalted butter, melted\n",
      "- 2 large eggs\n",
      "- 4 cups all-purpose flour\n",
      "- Vegetable oil for frying\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, dissolve the yeast in warm water and let it sit for 5 minutes until it becomes frothy.\n",
      "\n",
      "2. In a large mixing bowl, combine the warm milk, sugar, salt, melted butter, eggs, and yeast mixture.\n",
      "\n",
      "3. Gradually add the flour to the wet ingredients, stirring until a sticky dough forms.\n",
      "\n",
      "4. Turn the dough out onto a floured surface and knead for 5-7 minutes, adding more flour as needed until the dough is smooth and elastic.\n",
      "\n",
      "5. Place the dough in a greased bowl, cover with a clean kitchen towel, and let it rise in a warm place for 1-2 hours, or until doubled in size.\n",
      "\n",
      "6. Once the dough has risen, punch it down and roll it out on a floured surface to 1/2 inch thickness.\n",
      "\n",
      "7. Use a doughnut cutter or a round cookie cutter to cut out doughnut shapes. Use a smaller cutter to make the center holes.\n",
      "\n",
      "8. Place the doughnuts on a floured baking sheet, cover with a towel, and let them rise for another 30-45 minutes.\n",
      "\n",
      "9. Heat vegetable oil in a deep fryer or large pot to 375°F (190°C).\n",
      "\n",
      "10. Carefully place the doughnuts in the hot oil, frying for 1-2 minutes on each side until golden brown.\n",
      "\n",
      "11. Remove the doughnuts from the oil and place them on a paper towel-lined plate to drain.\n",
      "\n",
      "12. While the doughnuts are still warm, you can coat them in sugar, glaze, or icing of your choice.\n",
      "\n",
      "13. Enjoy your freshly made doughnuts!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a how to make doughnut.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10720a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make doughnuts at home, follow these steps:\n",
      "\n",
      "Ingredients:\n",
      "- 2 ¼ teaspoons active dry yeast\n",
      "- ½ cup warm water\n",
      "- ½ cup warm milk\n",
      "- ¼ cup granulated sugar\n",
      "- 1 teaspoon salt\n",
      "- 2 eggs\n",
      "- 1/3 cup butter, melted\n",
      "- 4 cups all-purpose flour\n",
      "- Vegetable oil for frying\n",
      "\n",
      "Instructions:\n",
      "1. In a small bowl, dissolve the yeast in warm water and let it sit for about 5 minutes until it becomes frothy.\n",
      "2. In a large mixing bowl, combine the yeast mixture, warm milk, sugar, salt, eggs, and melted butter. Mix well.\n",
      "3. Gradually add the flour to the wet ingredients and mix until a soft dough forms.\n",
      "4. Knead the dough on a lightly floured surface for about 5-7 minutes until it is smooth and elastic.\n",
      "5. Place the dough in a greased bowl, cover with a clean towel, and let it rise in a warm place for about 1 hour or until it has doubled in size.\n",
      "6. Once the dough has risen, punch it down and roll it out on a floured surface to about ½ inch thickness.\n",
      "7. Use a doughnut cutter or a round cookie cutter to cut out doughnut shapes. You can also use a smaller cutter to cut out the center hole.\n",
      "8. Place the doughnuts on a baking sheet lined with parchment paper and let them rise again for about 30 minutes.\n",
      "9. Heat the vegetable oil in a deep fryer or large pot to 375°F (190°C).\n",
      "10. Carefully add the doughnuts to the hot oil, frying them for about 1-2 minutes on each side until they are golden brown.\n",
      "11. Remove the doughnuts from the oil and place them on a paper towel-lined plate to drain excess oil.\n",
      "12. While the doughnuts are still warm, you can dip them in glaze, icing, or cinnamon sugar for added flavor.\n",
      "13. Enjoy your homemade doughnuts with a cup of coffee or tea!\n",
      "\n",
      "Note: Be careful when handling hot oil and fry the doughnuts in small batches to ensure they cook evenly."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623bcae",
   "metadata": {},
   "source": [
    "###Prompt Templates\n",
    "\n",
    "- A prompt refers to the input to model\n",
    "- Prompt templates are a way to create dynamic prompts for LLMs\n",
    "- A prompt template takes a piece of text and injects a user's inut into that piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0eec750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You are an experience virologist.\\nWrite a few sentences about the following \"Hepatitis\" in pidgin\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ''' You are an experience virologist.\n",
    "Write a few sentences about the following \"{virus}\" in {language}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(virus='Hepatitis', language='pidgin')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82c86aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatitis na serious sickness wey dey affect liver. E fit dey caused by virus, alcohol, or drugs. E fit dey cause yellow eyes, tiredness, and even liver failure if e no dey treated quick quick. Make sure say you dey protect yourself from hepatitis by dey avoid risky behaviors like sharing needles or having unprotected sex."
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "# output = llm.invoke(prompt)\n",
    "# print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a87ab7",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates\n",
    "- ChatPromptTemplates are specifically designed for tasks that involve in engaging conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ebc1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You respond in the JSON format\n",
      "Human: Top 5 countries in Africa by population.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond in the JSON format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format(n='5', area='Africa')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d059f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"name\": \"Nigeria\",\n",
      "            \"population\": \"206,139,589\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Ethiopia\",\n",
      "            \"population\": \"114,963,588\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Egypt\",\n",
      "            \"population\": \"102,334,404\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"DR Congo\",\n",
      "            \"population\": \"89,561,403\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Tanzania\",\n",
      "            \"population\": \"66,691,832\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3aa1a4",
   "metadata": {},
   "source": [
    "#### Chains\n",
    "Chains are like series of steps and actions. And each step can involve talking language models using a tool.\n",
    "Chains allow us to combine multiple components together to solve a specific task and build an entire LLM application.\n",
    "Langchain provides a standard interface for chains and some implementation to make it easy to use chains.\n",
    "Example of chains are:\n",
    "- Simple chain\n",
    "- Sequential chain\n",
    "- Custom chain\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77717c",
   "metadata": {},
   "source": [
    "#### Simple chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1f362fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an experience virologist.\n",
      "Write a few sentences about the following \"Ebola\" in english\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'language': 'english', 'virus': 'Ebola', 'text': 'Ebola is a highly infectious virus that causes severe illness in humans and other primates. It is transmitted through direct contact with the blood, body fluids, and tissues of infected individuals. Symptoms of Ebola include fever, severe headache, muscle pain, weakness, diarrhea, and vomiting. The virus has caused several outbreaks in Africa, with the most recent outbreak occurring in the Democratic Republic of Congo in 2020. Effective containment measures, including isolation of cases, contact tracing, and safe burial practices, are crucial in preventing the spread of Ebola.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = ''' You are an experience virologist.\n",
    "Write a few sentences about the following \"{virus}\" in {language}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'language': 'english', 'virus': 'Ebola'})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9743e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter country name:  Nigeria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of a Nigeria? List top 5 places to visit in the city. Use bullet points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Nigeria is Abuja.\n",
      "\n",
      "Top 5 places to visit in Abuja:\n",
      "- Aso Rock\n",
      "- Zuma Rock\n",
      "- National Mosque\n",
      "- National Arboretum\n",
      "- Jabi Lake\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "template = 'What is the capital of a {country}? List top 5 places to visit in the city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter country name: ')\n",
    "output = chain.invoke(country)\n",
    "\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07321631",
   "metadata": {},
   "source": [
    "#### Sequential Chain\n",
    "With sequential chains, you can make a series of calls to one more LLMs. You can take the output from one chain and \n",
    "use it as input to another chain.\n",
    "- Simple sequential chain - represent a series of chains, where each individual chain has a single input and a single output,\n",
    "  and output of one is used as input for the next step.\n",
    "- General form of sequential chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7aaded04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mIn Python, a thread is a separate flow of execution within a program. Threads allow a program to perform multiple tasks concurrently, making it possible to run multiple operations simultaneously. \n",
      "\n",
      "Here is a simple Python function that demonstrates the concept of threads using the `threading` module:\n",
      "\n",
      "```python\n",
      "import threading\n",
      "import time\n",
      "\n",
      "def print_numbers():\n",
      "    for i in range(5):\n",
      "        print(i)\n",
      "        time.sleep(1)\n",
      "\n",
      "def main():\n",
      "    # Create a new thread\n",
      "    t = threading.Thread(target=print_numbers)\n",
      "    \n",
      "    # Start the thread\n",
      "    t.start()\n",
      "    \n",
      "    # Continue executing the main program\n",
      "    for i in range(5, 10):\n",
      "        print(i)\n",
      "        time.sleep(1)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this example, we define a function `print_numbers` that prints numbers from 0 to 4 with a delay of 1 second between each number. We then create a new thread using `threading.Thread` and pass the `print_numbers` function as the target. We start the thread using `t.start()` and continue executing the main program, printing numbers from 5 to 9 with the same delay.\n",
      "\n",
      "By running this program, you will see the numbers printed by the main program and the thread interleaved, demonstrating how threads can run concurrently within a Python program.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mWhen the program is executed, the `main()` function is called. Within the `main()` function, we create a new thread using the threading module in Python. The `Thread` class from the `threading` module takes the `target` argument, which is the function to be executed in the new thread. In this case, we pass the `print_numbers` function as the target function.\n",
      "\n",
      "After creating the thread, we start it by calling the `start()` method on the thread object `t`. This causes the `print_numbers` function to be executed concurrently in a separate thread while the main program continues to execute.\n",
      "\n",
      "The `print_numbers` function runs in the new thread and prints numbers from 0 to 4 with a delay of 1 second between each number using the `time.sleep(1)` function. Meanwhile, the main program continues to execute in parallel, printing numbers from 5 to 9 also with a delay of 1 second between each number.\n",
      "\n",
      "As a result, you will see the numbers printed by the main program and the thread interleaved on the console. The two threads run concurrently, allowing multiple tasks to be performed at the same time. This demonstrates the concept of multithreading in Python, where multiple threads can execute their tasks simultaneously, improving the program's overall performance, especially for tasks that involve I/O operations or waiting for external events.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'thread', 'output': \"When the program is executed, the `main()` function is called. Within the `main()` function, we create a new thread using the threading module in Python. The `Thread` class from the `threading` module takes the `target` argument, which is the function to be executed in the new thread. In this case, we pass the `print_numbers` function as the target function.\\n\\nAfter creating the thread, we start it by calling the `start()` method on the thread object `t`. This causes the `print_numbers` function to be executed concurrently in a separate thread while the main program continues to execute.\\n\\nThe `print_numbers` function runs in the new thread and prints numbers from 0 to 4 with a delay of 1 second between each number using the `time.sleep(1)` function. Meanwhile, the main program continues to execute in parallel, printing numbers from 5 to 9 also with a delay of 1 second between each number.\\n\\nAs a result, you will see the numbers printed by the main program and the thread interleaved on the console. The two threads run concurrently, allowing multiple tasks to be performed at the same time. This demonstrates the concept of multithreading in Python, where multiple threads can execute their tasks simultaneously, improving the program's overall performance, especially for tasks that involve I/O operations or waiting for external events.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced python programmer. Write a function that explains the concept of {concept}'\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(llm=llm1, prompt=prompt_template_1)\n",
    "\n",
    "\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    template='Given the function{function}. explain it as deep as possible'\n",
    ")\n",
    "\n",
    "llm_2 =ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "chain_2 = LLMChain(llm=llm_2, prompt=prompt_template_2)\n",
    "\n",
    "overall_chain  = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('thread')\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ff6c4",
   "metadata": {},
   "source": [
    "#### Langchain Agents\n",
    "Large Language Models are powerful but lack the ability to perform certain tasks that even the simplest of apllication can do easiy.\n",
    "For example, LLMS struggle with calculation, logic or communicating other external components. the solution to this problem is to use agents.\n",
    "Agents are enabling tool for LLMs.\n",
    "Agents combine langchain tools and chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3eb3ee55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 156, 162, 168, 174, 180, 186, 192, 198]\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n*2 for n in range(100) if n%3 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fc804c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can first calculate the factorial of 12 and then find the square root of that result. Finally, we can display it with 4 decimal places.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe need to import the math module to access the square root function.\n",
      "Action: Python_REPL\n",
      "Action Input: math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe have calculated the factorial of 12, now let's find the square root of that.\n",
      "Action: Python_REPL\n",
      "Action Input: round(math.sqrt(math.factorial(12)), 4)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe have found the square root of the factorial of 12 and displayed it with 4 decimal places.\n",
      "Final Answer: 83.0454\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of factorial of 12 and display it in a 4 decimal places',\n",
       " 'output': '83.0454'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "agent_excutor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "agent_excutor.invoke('Calculate the square root of factorial of 12 and display it in a 4 decimal places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af533bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mPermutation of 5 means arranging 5 elements in a specific order.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe can use the math module in Python to calculate permutations.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a typo, it should be math.perm instead of math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake, the correct function is math.perm() not math.perm.\n",
      "Action: Python_REPL\n",
      "Action Input: math.perm(5, 5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Calculate the permutation 0f 5', 'output': 'Agent stopped due to iteration limit or time limit.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_excutor.invoke('Calculate the permutation 0f 5')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76734a81",
   "metadata": {},
   "source": [
    "#### langchain tools: DuckDuckgo and Wikipedia\n",
    "Langchain tools are like spcialized apps for your LLM. They are tiny code modules that allow it to access information.\n",
    "These tools are used to connect LLMs to search engines, databases, APIs, etc thereby expanding its knowledge and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b583b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "212c438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search  = DuckDuckGoSearchRun()\n",
    "# output = search.invoke('how to make whip cream')\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb11a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search  = DuckDuckGoSearchResults()\n",
    "# output = search.invoke('how to make whip cream step by step')\n",
    "output = search.run('Fred Mercury and QUeen')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='de-de', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Berlin')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd079df-59c6-4c6d-96d1-28ba4759aec6",
   "metadata": {},
   "source": [
    "#### Reasoning and Acting(ReAct)\n",
    "ReAct is a new approach that combines reasoning (chain-of-thoughts prompting) and acting capabilities of LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c89d2a-1a7c-4790-9af8-fe93038c3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c696fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool,AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "template ='''\n",
    "Answer the following questions as best as you can.\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "\n",
    "# Python REPL Tool (for executing python code)\n",
    "python_repl = PythonAstREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python Code'\n",
    ")\n",
    "\n",
    "# Wikipedia Tool(for searching wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or prson on Wikipedia.'\n",
    ")\n",
    "\n",
    "#DuckDuckGo Search Tool (for genearl web searches)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo',\n",
    "    func=search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "agent = create_react_agent(llm=llm,tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0380b95-87e8-4351-8d1b-0be8ccd1843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967cd1b",
   "metadata": {},
   "source": [
    "#### Embeddings Applications\n",
    "Embeddings are the core of buildings of LLMs applications.\n",
    "Text embeddings are numeric representation of text and are used in NLP and ML tasks.\n",
    "\n",
    "The distance between two embeddings or two vectors measures their relatedness which translates to the relatedness beween the text concepts they represent.\n",
    "Smimilar embeddings or vectore represent similar concepts.\n",
    "Usage Examples:\n",
    "- Text classification: assigning a label to a piece of text.\n",
    "- Text clustering: grouping together pieces of text that are similar in meaning.\n",
    "- Question-Answering: answering a question posed in natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cab0e",
   "metadata": {},
   "source": [
    "#### Vector Databases\n",
    "Vector databases are a new type of database designed staore and query unstructured data.\n",
    "\n",
    "- Unstructured data is data thta does not have a fixed schema, such as text, images, and audio.\n",
    "- Vector databases use a combination of different optimized algorithms that all participate in Approximate Nearest Neighbor(ANN) search\n",
    "\n",
    "Steps:\n",
    "1. Embedding\n",
    "2. Indexing\n",
    "3. Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c7f47",
   "metadata": {},
   "source": [
    "#### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ee393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a473580",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICONE_API_KEY = os.environ.get('PINOCONE_API_KEY')\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PICONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68f977-6d1a-47e8-bf46-1d1d734b138b",
   "metadata": {},
   "source": [
    "##### Pinecone Indexes\n",
    "An index is the highest-level organizational unit of vector data in Pinecone.\n",
    "\n",
    "It accepts and stores vectors, serves queries over the vectors it contains, and does other vector operations over its contents.\n",
    "\n",
    "- Serverless indexes: you dont configure or manage any compute or storage resources(they scale automatically)\n",
    "- Pod-based indexes: you choose one or more pre-configured units of hardware(pods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e935c7e",
   "metadata": {},
   "source": [
    "#### Working with Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import PodSpec\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(f\"Deleting index {index_name}\")\n",
    "    pc.delete_index(index_name)\n",
    "    print('Index deleted: :smile')\n",
    "else:\n",
    "    print(f'index {index_name} does not exists!')\n",
    "    \n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating index {index_name}\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=PodSpec(environment=\"gcp-starter\"),\n",
    "    )\n",
    "    print('Index created: :smile')\n",
    "else:\n",
    "    print(f'index {index_name} already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa579e",
   "metadata": {},
   "source": [
    "##### Working with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vectors\n",
    "import random\n",
    "\n",
    "vectors = [[random.random() for _  in range(1536)] for v in range(5)]\n",
    "\n",
    "# print(vectors)\n",
    "ids = list('abcde')\n",
    "index_name = 'langchain'\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert(vectors=zip(ids, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update vectors\n",
    "\n",
    "index.upsert(vectors=[('c', [0.3]*1536)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching queries\n",
    "\n",
    "# index.fetch(ids=['a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting index\n",
    "\n",
    "# index.delete(ids=['c', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd50f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "\n",
    "query_vector = [random.random() for v in range(1536)]\n",
    "index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=3,\n",
    "    include_values=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d820f",
   "metadata": {},
   "source": [
    "#### Namespaces\n",
    "Pinecone allows you to partition the vectors in an index into namespaces\n",
    "Queries and other operations are scoped to a specific namespace, allwing different request to search different subsets of your index.\n",
    "\n",
    "- Every index consist of one or more namespaces\n",
    "- Each vector exists in exactly one name space.\n",
    "- Namespace are uniquely identified by its name.\n",
    "- The default namespace is represented by empty string and is used if no namespace is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d98fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list('xyz')\n",
    "vectors = [[random.random() for _ in range(1536)] for v in range(len(ids))]\n",
    "index = pc.Index('langchain')\n",
    "index.upsert(vectors=zip(ids, vectors), namespace='first-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()\n",
    "# index.fetch(ids=['x'], namespace='first-namespace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting record from a namespace\n",
    "\n",
    "index.delete(ids=['x'], namespace='first-namespace')\n",
    "\n",
    "# delete all records in namespace\n",
    "index.delete(delete_all=True, namespace='first-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02efc5",
   "metadata": {},
   "source": [
    "#### Splitting and embedding text using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open('files/churchill_speech.txt') as f:\n",
    "    churchhill_speech = f.read()\n",
    "    \n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=100,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len\n",
    "# )\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ebffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents([churchhill_speech])\n",
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b9b9f",
   "metadata": {},
   "source": [
    "#### Embedding Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89852d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "calculate_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e17fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(chunks[0].page_content)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5ddbe",
   "metadata": {},
   "source": [
    "##### Inserting the Embeddings into a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6babcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "pc = pinecone.Pinecone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pc.list_indexes().names():\n",
    "    print('deleting all indexes .....', end='')\n",
    "    pc.delete_index(i)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'churchill-speech'\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating index {index_name}\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=PodSpec(environment=\"gcp-starter\"),\n",
    "    )\n",
    "    print('Index created: :smile')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to load from existing index\n",
    "vector_store = Pinecone.from_existing_index(index_name='churchill-speech', embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de36ea",
   "metadata": {},
   "source": [
    "##### Asking Questions(Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8110bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Where do we fight each other'\n",
    "result = vector_store.similarity_search(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in result:\n",
    "    print(res.page_content)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'Where do we fight each other'\n",
    "# query = 'Who was the king of Belgium at that time?'\n",
    "query = 'What about the French Armies?'\n",
    "answer = chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e8fd0",
   "metadata": {},
   "source": [
    "#### Google Gemini\n",
    "\n",
    "Gemini is a family of highly capable multimodal models developed by Google DeepMind.\n",
    "A multimodal model is an AI model that can process and understand information from multiple sources, such as text, audio, and video.\n",
    "---\n",
    "Gemini was tranined on a massive dataset of images, audio, video, text and code.\n",
    "- Geninin Ultra - largest and most capable model\n",
    "- Genini Pro -  Best for scalling\n",
    "- Gemini Nano - I.8B and 3.2B parameters. The most efficient for on device deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb0dcb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip  install -q langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d65dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-google-genai\n",
      "Version: 0.0.11\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/taiwosokunbi/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: google-generativeai, langchain-core\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ad86178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71f22c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "803e062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if 'GOOGLE_API_KEY' not in os.environ:\n",
    "    os.environ['GOOGLE_API_KEY'] = getpass.getpass('Provide your Google API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d0a2121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n",
      "models/embedding-001\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92b84a",
   "metadata": {},
   "source": [
    "#### Integrating Gemini with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4de272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatChatGoogleGenerativeAI(model='gemini-pro', temperature=0.9)\n",
    "response = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
